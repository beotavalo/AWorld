{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to AWorld\u2019s Documentation! Quickstart Agent Construction Install Multi-agent System Construction Workflow Construction","title":"Welcome to AWorld\u2019s Documentation!"},{"location":"#welcome-to-aworlds-documentation","text":"","title":"Welcome to AWorld\u2019s Documentation!"},{"location":"#quickstart","text":"Agent Construction Install Multi-agent System Construction Workflow Construction","title":"Quickstart"},{"location":"Quickstart/agent_construction/","text":"Building and Running Agents In AWorld's design, both Workflows and Multi-Agent Systems (MAS) are complex systems built around Agents as the core component. Using the most common llm_agent as an example, this tutorial provides detailed guidance on: How to quickly build an Agent How to customize an Agent This document is divided into two parts to explain AWorld's design philosophy. Part 1: Quick Agent Setup Declaring an Agent from aworld.agents.llm_agent import Agent # Assign a name to your agent agent = Agent(name=\"my_agent\") Configuring LLM Method 1: Using Environment Variables import os ## Set up LLM service using environment variables os.environ[\"LLM_PROVIDER\"] = \"openai\" # Choose from: openai, anthropic, azure_openai os.environ[\"LLM_MODEL_NAME\"] = \"gpt-4\" os.environ[\"LLM_API_KEY\"] = \"your-api-key\" os.environ[\"LLM_BASE_URL\"] = \"https://api.openai.com/v1\" # Optional for OpenAI Method 2: Using AgentConfig import os from aworld.agents.llm_agent import Agent from aworld.config.conf import AgentConfig agent_config = AgentConfig( llm_provider=os.getenv(\"LLM_PROVIDER\", \"openai\"), llm_model_name=os.getenv(\"LLM_MODEL_NAME\"), llm_base_url=os.getenv(\"LLM_BASE_URL\"), llm_api_key=os.getenv(\"LLM_API_KEY\"), ) agent = Agent(name=\"my_agent\", conf=agent_config) Method 3: Using Shared ModelConfig When multiple agents use the same LLM service, you can specify a shared ModelConfig: import os from aworld.agents.llm_agent import Agent from aworld.config.conf import AgentConfig, ModelConfig # Create a shared model configuration model_config = ModelConfig( llm_provider=os.getenv(\"LLM_PROVIDER\", \"openai\"), llm_model_name=os.getenv(\"LLM_MODEL_NAME\"), llm_base_url=os.getenv(\"LLM_BASE_URL\"), llm_api_key=os.getenv(\"LLM_API_KEY\"), ) # Use the shared model config in agent configuration agent_config = AgentConfig( llm_config=model_config, ) agent = Agent(name=\"my_agent\", conf=agent_config) Configuring Prompts from aworld.agents.llm_agent import Agent import os from aworld.config.conf import AgentConfig, ModelConfig model_config = ModelConfig( llm_provider=os.getenv(\"LLM_PROVIDER\", \"openai\"), llm_model_name=os.getenv(\"LLM_MODEL_NAME\"), llm_base_url=os.getenv(\"LLM_BASE_URL\"), llm_api_key=os.getenv(\"LLM_API_KEY\"), ) agent_config = AgentConfig( llm_config=model_config, ) # Define your system prompt system_prompt = \"\"\"You are a helpful AI assistant that can assist users with various tasks. You should be polite, accurate, and provide clear explanations.\"\"\" agent = Agent( name=\"my_agent\", conf=agent_config, system_prompt=system_prompt ) Configuring Tools Local Tools from aworld.agents.llm_agent import Agent import os from aworld.config.conf import AgentConfig, ModelConfig from aworld.core.tool.func_to_tool import be_tool model_config = ModelConfig( llm_provider=os.getenv(\"LLM_PROVIDER\", \"openai\"), llm_model_name=os.getenv(\"LLM_MODEL_NAME\"), llm_base_url=os.getenv(\"LLM_BASE_URL\"), llm_api_key=os.getenv(\"LLM_API_KEY\"), ) agent_config = AgentConfig( llm_config=model_config, ) system_prompt = \"\"\"You are a helpful agent with access to various tools.\"\"\" # Define a local tool using the @be_tool decorator @be_tool(tool_name='greeting_tool', tool_desc=\"A simple greeting tool that returns a hello message\") def greeting_tool() -> str: return \"Hello, world!\" agent = Agent( name=\"my_agent\", conf=agent_config, system_prompt=system_prompt, tool_names=['greeting_tool'] ) MCP (Model Context Protocol) Tools from aworld.agents.llm_agent import Agent import os from aworld.config.conf import AgentConfig, ModelConfig model_config = ModelConfig( llm_provider=os.getenv(\"LLM_PROVIDER\", \"openai\"), llm_model_name=os.getenv(\"LLM_MODEL_NAME\"), llm_base_url=os.getenv(\"LLM_BASE_URL\"), llm_api_key=os.getenv(\"LLM_API_KEY\"), ) agent_config = AgentConfig( llm_config=model_config, ) system_prompt = \"\"\"You are a helpful agent with access to file system operations.\"\"\" # Configure MCP servers mcp_config = { \"mcpServers\": { \"GorillaFileSystem\": { \"type\": \"stdio\", \"command\": \"python\", \"args\": [\"examples/BFCL/mcp_tools/gorilla_file_system.py\"], }, } } agent = Agent( name=\"my_agent\", conf=agent_config, system_prompt=system_prompt, mcp_servers=list(mcp_config.get(\"mcpServers\", {}).keys()), mcp_config=mcp_config ) Agent as Tool from aworld.agents.llm_agent import Agent import os from aworld.config.conf import AgentConfig, ModelConfig model_config = ModelConfig( llm_provider=os.getenv(\"LLM_PROVIDER\", \"openai\"), llm_model_name=os.getenv(\"LLM_MODEL_NAME\"), llm_base_url=os.getenv(\"LLM_BASE_URL\"), llm_api_key=os.getenv(\"LLM_API_KEY\"), ) agent_config = AgentConfig( llm_config=model_config, ) system_prompt = \"\"\"You are a helpful agent that can delegate tasks to other specialized agents.\"\"\" # Create a specialized tool agent tool_agent = Agent(name=\"tool_agent\", conf=agent_config) # Create the main agent that can use the tool agent agent = Agent( name=\"my_agent\", conf=agent_config, system_prompt=system_prompt, agent_names=['tool_agent'] ) Part 2: Customizing Agents Customizing Agent Input Override the init_observation() function to customize how your agent processes initial observations: async def init_observation(self, observation: Observation) -> Observation: # You can add extended information from other agents or third-party storage # For example, enrich the observation with additional context observation.metadata = {\"timestamp\": time.time(), \"source\": \"custom\"} return observation Customizing Model Input Override the async_messages_transform() function to customize how messages are transformed before being sent to the model: async def async_messages_transform(self, image_urls: List[str] = None, observation: Observation = None, message: Message = None, **kwargs) -> List[Dict[str, Any]]: \"\"\" Transform input data into the format expected by the LLM. Args: image_urls: List of images encoded using base64 observation: Observation from the environment message: Event received by the Agent \"\"\" messages = [] # Add system context if hasattr(self, 'system_prompt'): messages.append({\"role\": \"system\", \"content\": self.system_prompt}) # Add user message if message and message.content: messages.append({\"role\": \"user\", \"content\": message.content}) # Add images if present if image_urls: for img_url in image_urls: messages.append({ \"role\": \"user\", \"content\": [{\"type\": \"image_url\", \"image_url\": {\"url\": img_url}}] }) return messages Customizing Model Logic Override the invoke_model() function to implement custom model logic: async def invoke_model(self, messages: List[Dict[str, str]] = [], message: Message = None, **kwargs) -> ModelResponse: \"\"\"Custom model invocation logic. You can use neural networks, rule-based systems, or any other business logic. \"\"\" # Example: Use a custom model or business logic if self.use_custom_logic: # Your custom logic here response_content = self.custom_model.predict(messages) else: # Use the default LLM response_content = await self.llm_client.chat_completion(messages) return ModelResponse( id=f\"response_{int(time.time())}\", model=self.model_name, content=response_content, tool_calls=None # Set if tool calls are present ) Customizing Model Output Create a custom ModelOutputParser class and specify it using the model_output_parser parameter: from aworld.models.model_output_parser import ModelOutputParser class CustomOutputParser(ModelOutputParser[ModelResponse, AgentResult]): async def parse(self, resp: ModelResponse, **kwargs) -> AgentResult: \"\"\"Custom parsing logic based on your model's API response format.\"\"\" # Extract relevant information from the model response content = resp.content tool_calls = resp.tool_calls # Create your custom AgentResult result = AgentResult( content=content, tool_calls=tool_calls, metadata={\"parsed_at\": time.time()} ) return result # Use the custom parser agent = Agent( name=\"my_agent\", conf=agent_config, model_output_parser=CustomOutputParser() ) Customizing Agent Response Override the async_post_run() function to customize how your agent responds: from aworld.core.message import Message class CustomMessage(Message): def __init__(self, content: str, custom_field: str = None): super().__init__(content=content) self.custom_field = custom_field async def async_post_run(self, policy_result: List[ActionModel], policy_input: Observation, message: Message = None) -> Message: \"\"\" Customize the agent's response after processing. \"\"\" # Process the policy result and create a custom response response_content = f\"Processed {len(policy_result)} actions\" custom_field = \"custom_value\" return CustomMessage( content=response_content, custom_field=custom_field ) Custom Response Parsing If the framework doesn't support your response structure, you can create a custom response parser: from aworld.runners import HandlerFactory from aworld.runners.default_handler import DefaultHandler # Define a custom handler name custom_name = \"custom_handler\" @HandlerFactory.register(name=custom_name) class CustomHandler(DefaultHandler): def is_valid_message(self, message: Message): \"\"\"Check if this handler should process the message.\"\"\" return message.category == custom_name async def _do_handle(self, message: Message) -> AsyncGenerator[Message, None]: \"\"\"Custom message processing logic.\"\"\" if not self.is_valid_message(message): return # Implement your custom message processing logic here processed_message = self.process_custom_message(message) yield processed_message # Use the custom handler agent = Agent( name=\"my_agent\", conf=agent_config, event_handler_name=custom_name ) Important Note: The custom_name variable value must remain consistent across your handler registration and agent configuration.","title":"Agent Construction"},{"location":"Quickstart/agent_construction/#building-and-running-agents","text":"In AWorld's design, both Workflows and Multi-Agent Systems (MAS) are complex systems built around Agents as the core component. Using the most common llm_agent as an example, this tutorial provides detailed guidance on: How to quickly build an Agent How to customize an Agent This document is divided into two parts to explain AWorld's design philosophy.","title":"Building and Running Agents"},{"location":"Quickstart/agent_construction/#part-1-quick-agent-setup","text":"","title":"Part 1: Quick Agent Setup"},{"location":"Quickstart/agent_construction/#declaring-an-agent","text":"from aworld.agents.llm_agent import Agent # Assign a name to your agent agent = Agent(name=\"my_agent\")","title":"Declaring an Agent"},{"location":"Quickstart/agent_construction/#configuring-llm","text":"","title":"Configuring LLM"},{"location":"Quickstart/agent_construction/#method-1-using-environment-variables","text":"import os ## Set up LLM service using environment variables os.environ[\"LLM_PROVIDER\"] = \"openai\" # Choose from: openai, anthropic, azure_openai os.environ[\"LLM_MODEL_NAME\"] = \"gpt-4\" os.environ[\"LLM_API_KEY\"] = \"your-api-key\" os.environ[\"LLM_BASE_URL\"] = \"https://api.openai.com/v1\" # Optional for OpenAI","title":"Method 1: Using Environment Variables"},{"location":"Quickstart/agent_construction/#method-2-using-agentconfig","text":"import os from aworld.agents.llm_agent import Agent from aworld.config.conf import AgentConfig agent_config = AgentConfig( llm_provider=os.getenv(\"LLM_PROVIDER\", \"openai\"), llm_model_name=os.getenv(\"LLM_MODEL_NAME\"), llm_base_url=os.getenv(\"LLM_BASE_URL\"), llm_api_key=os.getenv(\"LLM_API_KEY\"), ) agent = Agent(name=\"my_agent\", conf=agent_config)","title":"Method 2: Using AgentConfig"},{"location":"Quickstart/agent_construction/#method-3-using-shared-modelconfig","text":"When multiple agents use the same LLM service, you can specify a shared ModelConfig: import os from aworld.agents.llm_agent import Agent from aworld.config.conf import AgentConfig, ModelConfig # Create a shared model configuration model_config = ModelConfig( llm_provider=os.getenv(\"LLM_PROVIDER\", \"openai\"), llm_model_name=os.getenv(\"LLM_MODEL_NAME\"), llm_base_url=os.getenv(\"LLM_BASE_URL\"), llm_api_key=os.getenv(\"LLM_API_KEY\"), ) # Use the shared model config in agent configuration agent_config = AgentConfig( llm_config=model_config, ) agent = Agent(name=\"my_agent\", conf=agent_config)","title":"Method 3: Using Shared ModelConfig"},{"location":"Quickstart/agent_construction/#configuring-prompts","text":"from aworld.agents.llm_agent import Agent import os from aworld.config.conf import AgentConfig, ModelConfig model_config = ModelConfig( llm_provider=os.getenv(\"LLM_PROVIDER\", \"openai\"), llm_model_name=os.getenv(\"LLM_MODEL_NAME\"), llm_base_url=os.getenv(\"LLM_BASE_URL\"), llm_api_key=os.getenv(\"LLM_API_KEY\"), ) agent_config = AgentConfig( llm_config=model_config, ) # Define your system prompt system_prompt = \"\"\"You are a helpful AI assistant that can assist users with various tasks. You should be polite, accurate, and provide clear explanations.\"\"\" agent = Agent( name=\"my_agent\", conf=agent_config, system_prompt=system_prompt )","title":"Configuring Prompts"},{"location":"Quickstart/agent_construction/#configuring-tools","text":"","title":"Configuring Tools"},{"location":"Quickstart/agent_construction/#local-tools","text":"from aworld.agents.llm_agent import Agent import os from aworld.config.conf import AgentConfig, ModelConfig from aworld.core.tool.func_to_tool import be_tool model_config = ModelConfig( llm_provider=os.getenv(\"LLM_PROVIDER\", \"openai\"), llm_model_name=os.getenv(\"LLM_MODEL_NAME\"), llm_base_url=os.getenv(\"LLM_BASE_URL\"), llm_api_key=os.getenv(\"LLM_API_KEY\"), ) agent_config = AgentConfig( llm_config=model_config, ) system_prompt = \"\"\"You are a helpful agent with access to various tools.\"\"\" # Define a local tool using the @be_tool decorator @be_tool(tool_name='greeting_tool', tool_desc=\"A simple greeting tool that returns a hello message\") def greeting_tool() -> str: return \"Hello, world!\" agent = Agent( name=\"my_agent\", conf=agent_config, system_prompt=system_prompt, tool_names=['greeting_tool'] )","title":"Local Tools"},{"location":"Quickstart/agent_construction/#mcp-model-context-protocol-tools","text":"from aworld.agents.llm_agent import Agent import os from aworld.config.conf import AgentConfig, ModelConfig model_config = ModelConfig( llm_provider=os.getenv(\"LLM_PROVIDER\", \"openai\"), llm_model_name=os.getenv(\"LLM_MODEL_NAME\"), llm_base_url=os.getenv(\"LLM_BASE_URL\"), llm_api_key=os.getenv(\"LLM_API_KEY\"), ) agent_config = AgentConfig( llm_config=model_config, ) system_prompt = \"\"\"You are a helpful agent with access to file system operations.\"\"\" # Configure MCP servers mcp_config = { \"mcpServers\": { \"GorillaFileSystem\": { \"type\": \"stdio\", \"command\": \"python\", \"args\": [\"examples/BFCL/mcp_tools/gorilla_file_system.py\"], }, } } agent = Agent( name=\"my_agent\", conf=agent_config, system_prompt=system_prompt, mcp_servers=list(mcp_config.get(\"mcpServers\", {}).keys()), mcp_config=mcp_config )","title":"MCP (Model Context Protocol) Tools"},{"location":"Quickstart/agent_construction/#agent-as-tool","text":"from aworld.agents.llm_agent import Agent import os from aworld.config.conf import AgentConfig, ModelConfig model_config = ModelConfig( llm_provider=os.getenv(\"LLM_PROVIDER\", \"openai\"), llm_model_name=os.getenv(\"LLM_MODEL_NAME\"), llm_base_url=os.getenv(\"LLM_BASE_URL\"), llm_api_key=os.getenv(\"LLM_API_KEY\"), ) agent_config = AgentConfig( llm_config=model_config, ) system_prompt = \"\"\"You are a helpful agent that can delegate tasks to other specialized agents.\"\"\" # Create a specialized tool agent tool_agent = Agent(name=\"tool_agent\", conf=agent_config) # Create the main agent that can use the tool agent agent = Agent( name=\"my_agent\", conf=agent_config, system_prompt=system_prompt, agent_names=['tool_agent'] )","title":"Agent as Tool"},{"location":"Quickstart/agent_construction/#part-2-customizing-agents","text":"","title":"Part 2: Customizing Agents"},{"location":"Quickstart/agent_construction/#customizing-agent-input","text":"Override the init_observation() function to customize how your agent processes initial observations: async def init_observation(self, observation: Observation) -> Observation: # You can add extended information from other agents or third-party storage # For example, enrich the observation with additional context observation.metadata = {\"timestamp\": time.time(), \"source\": \"custom\"} return observation","title":"Customizing Agent Input"},{"location":"Quickstart/agent_construction/#customizing-model-input","text":"Override the async_messages_transform() function to customize how messages are transformed before being sent to the model: async def async_messages_transform(self, image_urls: List[str] = None, observation: Observation = None, message: Message = None, **kwargs) -> List[Dict[str, Any]]: \"\"\" Transform input data into the format expected by the LLM. Args: image_urls: List of images encoded using base64 observation: Observation from the environment message: Event received by the Agent \"\"\" messages = [] # Add system context if hasattr(self, 'system_prompt'): messages.append({\"role\": \"system\", \"content\": self.system_prompt}) # Add user message if message and message.content: messages.append({\"role\": \"user\", \"content\": message.content}) # Add images if present if image_urls: for img_url in image_urls: messages.append({ \"role\": \"user\", \"content\": [{\"type\": \"image_url\", \"image_url\": {\"url\": img_url}}] }) return messages","title":"Customizing Model Input"},{"location":"Quickstart/agent_construction/#customizing-model-logic","text":"Override the invoke_model() function to implement custom model logic: async def invoke_model(self, messages: List[Dict[str, str]] = [], message: Message = None, **kwargs) -> ModelResponse: \"\"\"Custom model invocation logic. You can use neural networks, rule-based systems, or any other business logic. \"\"\" # Example: Use a custom model or business logic if self.use_custom_logic: # Your custom logic here response_content = self.custom_model.predict(messages) else: # Use the default LLM response_content = await self.llm_client.chat_completion(messages) return ModelResponse( id=f\"response_{int(time.time())}\", model=self.model_name, content=response_content, tool_calls=None # Set if tool calls are present )","title":"Customizing Model Logic"},{"location":"Quickstart/agent_construction/#customizing-model-output","text":"Create a custom ModelOutputParser class and specify it using the model_output_parser parameter: from aworld.models.model_output_parser import ModelOutputParser class CustomOutputParser(ModelOutputParser[ModelResponse, AgentResult]): async def parse(self, resp: ModelResponse, **kwargs) -> AgentResult: \"\"\"Custom parsing logic based on your model's API response format.\"\"\" # Extract relevant information from the model response content = resp.content tool_calls = resp.tool_calls # Create your custom AgentResult result = AgentResult( content=content, tool_calls=tool_calls, metadata={\"parsed_at\": time.time()} ) return result # Use the custom parser agent = Agent( name=\"my_agent\", conf=agent_config, model_output_parser=CustomOutputParser() )","title":"Customizing Model Output"},{"location":"Quickstart/agent_construction/#customizing-agent-response","text":"Override the async_post_run() function to customize how your agent responds: from aworld.core.message import Message class CustomMessage(Message): def __init__(self, content: str, custom_field: str = None): super().__init__(content=content) self.custom_field = custom_field async def async_post_run(self, policy_result: List[ActionModel], policy_input: Observation, message: Message = None) -> Message: \"\"\" Customize the agent's response after processing. \"\"\" # Process the policy result and create a custom response response_content = f\"Processed {len(policy_result)} actions\" custom_field = \"custom_value\" return CustomMessage( content=response_content, custom_field=custom_field )","title":"Customizing Agent Response"},{"location":"Quickstart/agent_construction/#custom-response-parsing","text":"If the framework doesn't support your response structure, you can create a custom response parser: from aworld.runners import HandlerFactory from aworld.runners.default_handler import DefaultHandler # Define a custom handler name custom_name = \"custom_handler\" @HandlerFactory.register(name=custom_name) class CustomHandler(DefaultHandler): def is_valid_message(self, message: Message): \"\"\"Check if this handler should process the message.\"\"\" return message.category == custom_name async def _do_handle(self, message: Message) -> AsyncGenerator[Message, None]: \"\"\"Custom message processing logic.\"\"\" if not self.is_valid_message(message): return # Implement your custom message processing logic here processed_message = self.process_custom_message(message) yield processed_message # Use the custom handler agent = Agent( name=\"my_agent\", conf=agent_config, event_handler_name=custom_name ) Important Note: The custom_name variable value must remain consistent across your handler registration and agent configuration.","title":"Custom Response Parsing"},{"location":"Quickstart/install/","text":"Installation AWorld Agent Prerequisites Python 3.10+ Install git clone https://github.com/inclusionAI/AWorld && cd AWorld pip install . AWorld Env TODO AWorld Train TODO","title":"Install"},{"location":"Quickstart/install/#installation","text":"","title":"Installation"},{"location":"Quickstart/install/#aworld-agent","text":"","title":"AWorld Agent"},{"location":"Quickstart/install/#prerequisites","text":"Python 3.10+","title":"Prerequisites"},{"location":"Quickstart/install/#install","text":"git clone https://github.com/inclusionAI/AWorld && cd AWorld pip install .","title":"Install"},{"location":"Quickstart/install/#aworld-env","text":"TODO","title":"AWorld Env"},{"location":"Quickstart/install/#aworld-train","text":"TODO","title":"AWorld Train"},{"location":"Quickstart/multi-agent_system_construction/","text":"Building and Running Multi-Agent Systems (MAS) In the AWorld framework, similar to Workflow Construction, the fundamental building block for MAS is the Agent. By introducing the Swarm concept, users can easily, quickly, and efficiently build complex Multi-Agent Systems. In summary: Workflow in AWorld : Static, pre-defined execution flows MAS in AWorld : Dynamic, real-time decision-making execution flows This design ensures unified underlying capabilities (i.e., Agent, Graph-based Topology) while maintaining extensibility. Quick MAS Construction Similar to Workflows, we can easily define communication networks between Agents through topology. The key difference is that by using build_type=GraphBuildType.HANDOFF , we allow dynamic decision-making for inter-agent calling relationships: agent1 can selectively decide to call agent2 and agent3 ; the number of calls is also dynamic (once or multiple times) agent2 can selectively decide to call agent3 ; the number of calls is also dynamic (once or multiple times) from aworld.config.conf import AgentConfig from aworld.agents.llm_agent import Agent from aworld.core.agent.swarm import Swarm, GraphBuildType from aworld.runner import Runners # Configure agents agent_conf = AgentConfig(...) agent1 = Agent(name=\"agent1\", conf=agent_conf) agent2 = Agent(name=\"agent2\", conf=agent_conf) agent3 = Agent(name=\"agent3\", conf=agent_conf) # Create swarm with dynamic handoff topology swarm = Swarm( topology=[(agent1, agent2), (agent2, agent3), (agent1, agent3)], build_type=GraphBuildType.HANDOFF ) # Run the swarm Runners.run(input=\"your question\", swarm=swarm) Specifying Entry Agent Since MAS is essentially a Graph by definition, different Agents can accept external input. We can specify which Agent receives the query using the root_agent parameter. swarm = Swarm( topology=[(agent1, agent2), (agent2, agent3), (agent1, agent3)], build_type=GraphBuildType.HANDOFF, root_agent=[agent1] ) Dynamic Routing When the policy() function decides which agent to call next, for special cases, Agents may need customized routing based on specific business rules. You can override the handler in the corresponding Agent: # Handler name consistency must be maintained agent = Agent(..., event_handler_name=\"your_handler_name\") from aworld.core.handler import HandlerFactory, DefaultHandler from aworld.core.message import Message from typing import AsyncGenerator @HandlerFactory.register(name=\"your_handler_name\") class YourHandler(DefaultHandler): def is_valid_message(self, message: Message) -> bool: return message.category == \"your_handler_name\" async def _do_handle(self, message: Message) -> AsyncGenerator[Message, None]: if not self.is_valid_message(message): return # The type of data is generally ActionModel, but can be customized data = message.payload if \"clause1\" in data: # Handle clause1 logic pass elif \"clause2\" in data: # Handle clause2 logic pass You can refer to the implementation of DefaultTaskHandler in AWorld. Two Examples of Overriding Routing: ReAct and Plan-Execute ReAct @HandlerFactory.register(name='react') class ReactHandler(AgentHandler): def is_valid_message(self, message: Message): if message.category != 'react': return False return True async def _do_handle(self, message: Message) -> AsyncGenerator[Message, None]: yield message Plan-Execute Compared to ReAct, agent2 and agent3 can execute in parallel simultaneously. from aworld.core.common import Observation from aworld.core.event.base import AgentMessage from aworld.logs.util import logger @HandlerFactory.register(name='plan_execute') class PlanExecuteHandler(AgentHandler): def is_valid_message(self, message: Message): if message.category != 'plan_execute': return False return True async def _do_handle(self, message: Message) -> AsyncGenerator[Message, None]: logger.info(f\"PlanExecuteHandler|handle|taskid={self.task_id}|is_sub_task={message.context._task.is_sub_task}\") content = message.payload # Parse model plan plan = parse_plan(content[0].policy_info) logger.info(f\"PlanExecuteHandler|plan|{plan}\") # Execute steps output, context = execution_steps(plan.steps) # Send event message, notify the next processing agent new_plan_input = Observation(content=output) yield AgentMessage( session_id=message.session_id, payload=new_plan_input, sender=self.name(), receiver=self.swarm.communicate_agent.id(), headers={'context': context} ) For more details, refer to the examples. Combination and Recursion of MAS and Workflow Same or different types of Swarms can be deeply nested, providing multi-level Swarms with different interaction mechanisms to support complex multi-agent interactions. For example, when creating a travel itinerary planner, using a combination of Workflow + MAS, where Workflow provides deterministic processes and MAS handles multi-source information retrieval and integration. from aworld.config.conf import AgentConfig from aworld.agents.llm_agent import Agent from aworld.core.agent.swarm import Swarm, GraphBuildType # Configure agents agent_conf = AgentConfig(...) # Create five agents rewrite = Agent(name=\"rewrite\", conf=agent_conf) plan = Agent(name=\"plan\", conf=agent_conf) search = Agent(name=\"search\", conf=agent_conf) summary = Agent(name=\"summary\", conf=agent_conf) report = Agent(name=\"report\", conf=agent_conf) # Construct a MAS mas = Swarm( topology=[(plan, search), (plan, summary)], build_type=GraphBuildType.HANDOFF, root_agent=[plan] ) # Construct a combination of a workflow with the MAS team combination = Swarm( topology=[(rewrite, mas), (mas, report)], root_agent=[rewrite] )","title":"Multi-agent System Construction"},{"location":"Quickstart/multi-agent_system_construction/#building-and-running-multi-agent-systems-mas","text":"In the AWorld framework, similar to Workflow Construction, the fundamental building block for MAS is the Agent. By introducing the Swarm concept, users can easily, quickly, and efficiently build complex Multi-Agent Systems. In summary: Workflow in AWorld : Static, pre-defined execution flows MAS in AWorld : Dynamic, real-time decision-making execution flows This design ensures unified underlying capabilities (i.e., Agent, Graph-based Topology) while maintaining extensibility.","title":"Building and Running Multi-Agent Systems (MAS)"},{"location":"Quickstart/multi-agent_system_construction/#quick-mas-construction","text":"Similar to Workflows, we can easily define communication networks between Agents through topology. The key difference is that by using build_type=GraphBuildType.HANDOFF , we allow dynamic decision-making for inter-agent calling relationships: agent1 can selectively decide to call agent2 and agent3 ; the number of calls is also dynamic (once or multiple times) agent2 can selectively decide to call agent3 ; the number of calls is also dynamic (once or multiple times) from aworld.config.conf import AgentConfig from aworld.agents.llm_agent import Agent from aworld.core.agent.swarm import Swarm, GraphBuildType from aworld.runner import Runners # Configure agents agent_conf = AgentConfig(...) agent1 = Agent(name=\"agent1\", conf=agent_conf) agent2 = Agent(name=\"agent2\", conf=agent_conf) agent3 = Agent(name=\"agent3\", conf=agent_conf) # Create swarm with dynamic handoff topology swarm = Swarm( topology=[(agent1, agent2), (agent2, agent3), (agent1, agent3)], build_type=GraphBuildType.HANDOFF ) # Run the swarm Runners.run(input=\"your question\", swarm=swarm)","title":"Quick MAS Construction"},{"location":"Quickstart/multi-agent_system_construction/#specifying-entry-agent","text":"Since MAS is essentially a Graph by definition, different Agents can accept external input. We can specify which Agent receives the query using the root_agent parameter. swarm = Swarm( topology=[(agent1, agent2), (agent2, agent3), (agent1, agent3)], build_type=GraphBuildType.HANDOFF, root_agent=[agent1] )","title":"Specifying Entry Agent"},{"location":"Quickstart/multi-agent_system_construction/#dynamic-routing","text":"When the policy() function decides which agent to call next, for special cases, Agents may need customized routing based on specific business rules. You can override the handler in the corresponding Agent: # Handler name consistency must be maintained agent = Agent(..., event_handler_name=\"your_handler_name\") from aworld.core.handler import HandlerFactory, DefaultHandler from aworld.core.message import Message from typing import AsyncGenerator @HandlerFactory.register(name=\"your_handler_name\") class YourHandler(DefaultHandler): def is_valid_message(self, message: Message) -> bool: return message.category == \"your_handler_name\" async def _do_handle(self, message: Message) -> AsyncGenerator[Message, None]: if not self.is_valid_message(message): return # The type of data is generally ActionModel, but can be customized data = message.payload if \"clause1\" in data: # Handle clause1 logic pass elif \"clause2\" in data: # Handle clause2 logic pass You can refer to the implementation of DefaultTaskHandler in AWorld.","title":"Dynamic Routing"},{"location":"Quickstart/multi-agent_system_construction/#two-examples-of-overriding-routing-react-and-plan-execute","text":"","title":"Two Examples of Overriding Routing: ReAct and Plan-Execute"},{"location":"Quickstart/multi-agent_system_construction/#react","text":"@HandlerFactory.register(name='react') class ReactHandler(AgentHandler): def is_valid_message(self, message: Message): if message.category != 'react': return False return True async def _do_handle(self, message: Message) -> AsyncGenerator[Message, None]: yield message","title":"ReAct"},{"location":"Quickstart/multi-agent_system_construction/#plan-execute","text":"Compared to ReAct, agent2 and agent3 can execute in parallel simultaneously. from aworld.core.common import Observation from aworld.core.event.base import AgentMessage from aworld.logs.util import logger @HandlerFactory.register(name='plan_execute') class PlanExecuteHandler(AgentHandler): def is_valid_message(self, message: Message): if message.category != 'plan_execute': return False return True async def _do_handle(self, message: Message) -> AsyncGenerator[Message, None]: logger.info(f\"PlanExecuteHandler|handle|taskid={self.task_id}|is_sub_task={message.context._task.is_sub_task}\") content = message.payload # Parse model plan plan = parse_plan(content[0].policy_info) logger.info(f\"PlanExecuteHandler|plan|{plan}\") # Execute steps output, context = execution_steps(plan.steps) # Send event message, notify the next processing agent new_plan_input = Observation(content=output) yield AgentMessage( session_id=message.session_id, payload=new_plan_input, sender=self.name(), receiver=self.swarm.communicate_agent.id(), headers={'context': context} ) For more details, refer to the examples.","title":"Plan-Execute"},{"location":"Quickstart/multi-agent_system_construction/#combination-and-recursion-of-mas-and-workflow","text":"Same or different types of Swarms can be deeply nested, providing multi-level Swarms with different interaction mechanisms to support complex multi-agent interactions. For example, when creating a travel itinerary planner, using a combination of Workflow + MAS, where Workflow provides deterministic processes and MAS handles multi-source information retrieval and integration. from aworld.config.conf import AgentConfig from aworld.agents.llm_agent import Agent from aworld.core.agent.swarm import Swarm, GraphBuildType # Configure agents agent_conf = AgentConfig(...) # Create five agents rewrite = Agent(name=\"rewrite\", conf=agent_conf) plan = Agent(name=\"plan\", conf=agent_conf) search = Agent(name=\"search\", conf=agent_conf) summary = Agent(name=\"summary\", conf=agent_conf) report = Agent(name=\"report\", conf=agent_conf) # Construct a MAS mas = Swarm( topology=[(plan, search), (plan, summary)], build_type=GraphBuildType.HANDOFF, root_agent=[plan] ) # Construct a combination of a workflow with the MAS team combination = Swarm( topology=[(rewrite, mas), (mas, report)], root_agent=[rewrite] )","title":"Combination and Recursion of MAS and Workflow"},{"location":"Quickstart/workflow_construction/","text":"We use the classic graph syntax to describe workflows in AWorld. The following are the basic scenarios for constructing agent workflows. Agent Native Workflow Sequential \"\"\" Sequential Agent Pipeline: agent1 \u2192 agent2 \u2192 agent3 Executes agents in sequence where each agent's output becomes the next agent's input, enabling multi-step collaborative processing. \"\"\" swarm = Swarm([(agent1, agent2), (agent2, agent3)], root_agent=[agent1]) result: TaskResponse = Runners.run(input=question, swarm=swarm) Parallel \"\"\" Parallel Agent Execution with Barrier Synchronization Input \u2500\u2500\u252c\u2500\u2192 agent1 \u2500\u2500\u2510 \u2502 \u251c\u2500\u2500\u2192 agent3 (barrier wait) \u2514\u2500\u2192 agent2 \u2500\u2500\u2518 - agent1 and agent2 execute in parallel - agent3 acts as a barrier, waiting for both agents - agent3 processes combined outputs from agent1 and agent2 \"\"\" swarm = Swarm([(agent1, agent3), (agent2, agent3)], root_agent=[agent1, agent2]) result: TaskResponse = Runners.run(input=question, swarm=swarm) Parallel Multi-Path \"\"\" Parallel Multi-Path Agent Execution Input \u2500\u2500\u2192 agent1 \u2500\u2500\u252c\u2500\u2500\u2192 agent2 \u2500\u2500\u2510 \u2502 \u2502 \u2514\u2500\u2500\u2192 agent3 \u2190\u2500\u2518 (barrier wait for agent1 & agent2) - Single input enters only through agent1 - agent1 distributes to both agent2 and agent3 - agent2 processes and feeds agent3 - agent3 waits for both agent1 and agent2 completion - agent3 synthesizes outputs from both agent1 and agent2 \"\"\" swarm = Swarm([(agent1, agent2), (agent1, agent3), (agent2, agent3)], root_agent=[agent1]) result: TaskResponse = Runners.run(input=question, swarm=swarm) Task Native Workflow Task native workflow is further implemented for Isolating the agent runtimes and environments, in the distributed or other easy-to-overlap scenarios. Task native workflow is further implemented for isolating agent runtimes and environments, particularly useful in distributed or other scenarios where tool-isolation is required. task1 = Task(input=\"my question\", agent=agent1) task2 = Task(agent=agent2) task3 = Task(agent=agent3) tasks = [task1, task2, task3] result: Dict[str, TaskResponse] = Runners.run_task(tasks, RunConfig(sequence_dependent=True))","title":"Workflow Construction"},{"location":"Quickstart/workflow_construction/#agent-native-workflow","text":"","title":"Agent Native Workflow"},{"location":"Quickstart/workflow_construction/#sequential","text":"\"\"\" Sequential Agent Pipeline: agent1 \u2192 agent2 \u2192 agent3 Executes agents in sequence where each agent's output becomes the next agent's input, enabling multi-step collaborative processing. \"\"\" swarm = Swarm([(agent1, agent2), (agent2, agent3)], root_agent=[agent1]) result: TaskResponse = Runners.run(input=question, swarm=swarm)","title":"Sequential"},{"location":"Quickstart/workflow_construction/#parallel","text":"\"\"\" Parallel Agent Execution with Barrier Synchronization Input \u2500\u2500\u252c\u2500\u2192 agent1 \u2500\u2500\u2510 \u2502 \u251c\u2500\u2500\u2192 agent3 (barrier wait) \u2514\u2500\u2192 agent2 \u2500\u2500\u2518 - agent1 and agent2 execute in parallel - agent3 acts as a barrier, waiting for both agents - agent3 processes combined outputs from agent1 and agent2 \"\"\" swarm = Swarm([(agent1, agent3), (agent2, agent3)], root_agent=[agent1, agent2]) result: TaskResponse = Runners.run(input=question, swarm=swarm)","title":"Parallel"},{"location":"Quickstart/workflow_construction/#parallel-multi-path","text":"\"\"\" Parallel Multi-Path Agent Execution Input \u2500\u2500\u2192 agent1 \u2500\u2500\u252c\u2500\u2500\u2192 agent2 \u2500\u2500\u2510 \u2502 \u2502 \u2514\u2500\u2500\u2192 agent3 \u2190\u2500\u2518 (barrier wait for agent1 & agent2) - Single input enters only through agent1 - agent1 distributes to both agent2 and agent3 - agent2 processes and feeds agent3 - agent3 waits for both agent1 and agent2 completion - agent3 synthesizes outputs from both agent1 and agent2 \"\"\" swarm = Swarm([(agent1, agent2), (agent1, agent3), (agent2, agent3)], root_agent=[agent1]) result: TaskResponse = Runners.run(input=question, swarm=swarm)","title":"Parallel Multi-Path"},{"location":"Quickstart/workflow_construction/#task-native-workflow","text":"Task native workflow is further implemented for Isolating the agent runtimes and environments, in the distributed or other easy-to-overlap scenarios. Task native workflow is further implemented for isolating agent runtimes and environments, particularly useful in distributed or other scenarios where tool-isolation is required. task1 = Task(input=\"my question\", agent=agent1) task2 = Task(agent=agent2) task3 = Task(agent=agent3) tasks = [task1, task2, task3] result: Dict[str, TaskResponse] = Runners.run_task(tasks, RunConfig(sequence_dependent=True))","title":"Task Native Workflow"}]}